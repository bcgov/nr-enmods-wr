apiVersion: batch/v1
kind: CronJob
metadata:
  name: aqi-weekly-sync
  namespace: "{{ .Release.Namespace }}"
spec:
  schedule: "0 23 * * 1" #change this to every saturday at midnight
  timeZone: "America/Vancouver"
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: db-sync
              image: "{{.Values.global.registry}}/{{.Values.global.repository}}/custom-postgres-with-aws:{{ .Values.global.tag | default .Chart.AppVersion }}"
              env:
                # Database credentials from openshift secrets
                - name: POSTGRES_HOST
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: POSTGRES_HOST
                - name: POSTGRES_USER
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: POSTGRES_USER
                - name: POSTGRES_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: POSTGRES_PASSWORD
                - name: POSTGRES_DBNAME
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: POSTGRES_DATABASE

                # S3 Credentials from openshift secrets
                - name: S3_BUCKET
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: S3_BUCKET
                - name: S3_BUCKET_PATH
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: S3_BUCKET_PATH
                - name: S3_REGION
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: S3_REGION
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: AWS_ACCESS_KEY_ID
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: AWS_SECRET_ACCESS_KEY
              args:
                - /bin/bash
                - "-c"
                - |
                  set -e
                  trap 'catch $? $LINENO' ERR

                  # Logging function with timestamp and color
                  log() {
                    local type="$1"; shift
                    local color_reset="\033[0m"
                    local color_info="\033[1;36m"
                    local color_warn="\033[1;33m"
                    local color_error="\033[1;31m"
                    local ts="$(date '+%Y-%m-%d %H:%M:%S')"
                    case "$type" in
                      INFO) echo -e "${color_info}[INFO $ts] $*${color_reset}" ;;
                      WARN) echo -e "${color_warn}[WARN $ts] $*${color_reset}" ;;
                      ERROR) echo -e "${color_error}[ERROR $ts] $*${color_reset}" ;;
                      *) echo "[LOG $ts] $*" ;;
                    esac
                  }

                  catch() {
                    local exit_code=$1
                    local line_no=$2
                    log ERROR "Script failed with exit code $exit_code at line $line_no."
                    exit $exit_code
                  }

                  log INFO "Starting AQI weekly sync job..."

                  log INFO "Getting latest directory (prefix) from S3..."
                  if ! FOLDERS=$(aws s3api list-objects-v2 --bucket "$S3_BUCKET" --prefix "$S3_BUCKET_PATH" --delimiter '/' --query "CommonPrefixes[].Prefix" --output text); then
                    log ERROR "Failed to list objects in S3 bucket $S3_BUCKET with prefix $S3_BUCKET_PATH"
                    exit 1
                  fi
                  read -r -a folderList <<< "$FOLDERS"

                  LATEST_FOLDER=$(printf "%s\n" "${folderList[@]}" | sort | tail -n 1)

                  if [ -z "$LATEST_FOLDER" ]; then
                    log ERROR "No directories found under prefix $S3_BUCKET_PATH"
                    exit 1
                  fi

                  log INFO "Latest directory (prefix) is: $LATEST_FOLDER"
                  log INFO "Listing .csv.gz files inside the directory..."

                  if ! FILES=$(aws s3api list-objects-v2 --bucket "$S3_BUCKET" --prefix "$LATEST_FOLDER" --query "Contents[?ends_with(Key, '.csv.gz')].Key" --output text); then
                    log ERROR "Failed to list .csv.gz files in $LATEST_FOLDER"
                    exit 1
                  fi

                  if [ -z "$FILES" ]; then
                    log ERROR "No .csv.gz files found in $LATEST_FOLDER"
                    exit 1
                  fi

                  normalized=$(printf "%s" "$FILES" | tr '\t' '\n' | tr -d '\r' | sed '/^[[:space:]]*$/d')

                  # Convert into array safely
                  mapfile -t gzFilesList <<< "$normalized"

                  log INFO "Files after read:"
                  printf '%s\n' "${gzFilesList[@]}"
                  echo

                  # Get the latest synced folder name from database
                  log INFO "Getting latest synced folder from database..."
                  export DATABASE_URL="postgresql://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST/$POSTGRES_DBNAME"
                  LATEST_SYNCED_FOLDER=$(psql "$DATABASE_URL" -t -A -c "
                    SELECT source_folder 
                    FROM S3_SYNC_LOG 
                    WHERE status='SUCCESS' AND source_folder IS NOT NULL 
                    ORDER BY start_time DESC 
                    LIMIT 1;
                  ") || {
                    log ERROR "Failed to get latest synced folder from database."
                    exit 1
                  }

                  # Trim whitespace
                  LATEST_SYNCED_FOLDER=$(echo "$LATEST_SYNCED_FOLDER" | xargs)

                  # Now safely compare
                  if [[ -z "$LATEST_SYNCED_FOLDER" ]]; then
                    log INFO "No previous synced folder found (first sync). Proceeding with folder '$LATEST_FOLDER'."
                    
                    # Build SQL array
                    log INFO "Preparing S3 key array for SQL insertion..."
                    S3_KEY_ARRAY=$(
                      for key in "${gzFilesList[@]}"; do
                        safe_key=${key//\'/\'\'}
                        printf "'%s'," "$safe_key"
                      done | sed 's/,$//'
                    )

                    log INFO "Final SQL Array: ARRAY[$S3_KEY_ARRAY]"
                    log INFO "Running load from S3..."
                    if ! psql "$DATABASE_URL" \
                      -c "SELECT run_aqi_s3_load('$S3_BUCKET', ARRAY[$S3_KEY_ARRAY]::text[], '$LATEST_FOLDER' ,'$S3_REGION', NULL, '$AWS_ACCESS_KEY_ID', '$AWS_SECRET_ACCESS_KEY', '$AWS_SESSION_TOKEN');"; then
                      log ERROR "Function run_aqi_s3_load failed!"
                      exit 1
                    fi

                    log INFO "Data load from S3 completed successfully."
                    log INFO "Running table swap..."
                    if ! psql "$DATABASE_URL" \
                      -c "SELECT run_aqi_table_swap('AQI_AWS_S3_SYNC', '$LATEST_FOLDER');"; then
                      log ERROR "Function run_aqi_table_swap failed!"
                      exit 1
                    fi

                  elif [[ "$LATEST_FOLDER" < "$LATEST_SYNCED_FOLDER" || "$LATEST_FOLDER" == "$LATEST_SYNCED_FOLDER" ]]; then
                    log WARN "No new data to sync. Latest folder '$LATEST_FOLDER' is already synced or is older."
                    psql "$DATABASE_URL" \
                      -c "INSERT INTO S3_SYNC_LOG (process_name, status, error_message, source_folder, start_time, finish_time) VALUES ('AQI_AWS_S3_SYNC', 'SKIPPED_NO_CHANGE', 'No new data to sync. Latest folder $LATEST_FOLDER is already synced.', '$LATEST_FOLDER', now(), now());"
                  else
                    log INFO "New data found. Proceeding with sync for folder '$LATEST_FOLDER'."

                    # Build SQL array
                    log INFO "Preparing S3 key array for SQL insertion..."
                    S3_KEY_ARRAY=$(
                      for key in "${gzFilesList[@]}"; do
                        safe_key=${key//\'/\'\'}
                        printf "'%s'," "$safe_key"
                      done | sed 's/,$//'
                    )

                    log INFO "Final SQL Array: ARRAY[$S3_KEY_ARRAY]"
                    log INFO "Running load from S3..."
                    if ! psql "$DATABASE_URL" \
                      -c "SELECT run_aqi_s3_load('$S3_BUCKET', ARRAY[$S3_KEY_ARRAY]::text[], '$LATEST_FOLDER' ,'$S3_REGION', NULL, '$AWS_ACCESS_KEY_ID', '$AWS_SECRET_ACCESS_KEY', '$AWS_SESSION_TOKEN');"; then
                      log ERROR "Function run_aqi_s3_load failed!"
                      exit 1
                    fi

                    log INFO "Data load from S3 completed successfully."
                    log INFO "Running table swap..."
                    if ! psql "$DATABASE_URL" \
                      -c "SELECT run_aqi_table_swap('AQI_AWS_S3_SYNC', '$LATEST_FOLDER');"; then
                      log ERROR "Function run_aqi_table_swap failed!"
                      exit 1
                    fi

                  fi

                  log INFO "Table swap completed."
                  log INFO "AQI hourly sync completed."
          restartPolicy: OnFailure
