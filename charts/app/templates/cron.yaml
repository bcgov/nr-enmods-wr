apiVersion: batch/v1
kind: CronJob
metadata:
  name: aqi-weekly-sync
  namespace: d23a53-dev
spec:
  schedule: "0 0 * * 6" #change this to every saturday at midnight
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: db-sync
              image: ghcr.io/bcgov/nr-enmods-wr/custom-postgres-with-aws:101
              env:
                # Database credentials from openshift secrets
                - name: POSTGRES_HOST
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: POSTGRES_HOST
                - name: POSTGRES_USER
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: POSTGRES_USER
                - name: POSTGRES_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: POSTGRES_PASSWORD
                - name: POSTGRES_DBNAME
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: POSTGRES_DATABASE

                # S3 Credentials from openshift secrets
                - name: S3_BUCKET
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: S3_BUCKET
                - name: S3_BUCKET_PATH
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: S3_BUCKET_PATH
                - name: S3_REGION
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: S3_REGION
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: AWS_ACCESS_KEY_ID
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      name: "{{ .Release.Name }}-database"
                      key: AWS_SECRET_ACCESS_KEY
              args:
                - /bin/bash
                - '-c'
                - |
                  echo "Starting AQI weekly sync job..."

                  echo "Getting latest directory (prefix) from S3..."
                  FOLDERS=$(aws s3api list-objects-v2 --bucket "$S3_BUCKET" --prefix "$S3_BUCKET_PATH" --delimiter '/' --query "CommonPrefixes[].Prefix" --output text)
                  read -r -a folderList <<< "$FOLDERS"

                  LATEST_FOLDER=$(printf "%s\n" "${folderList[@]}" | sort | tail -n 1)

                  if [ -z "$LATEST_FOLDER" ]; then
                    echo "ERROR: No directories found under prefix $S3_BUCKET_PATH"
                    exit 1
                  fi

                  echo "Latest directory (prefix) is: $LATEST_FOLDER"
                  echo "Listing .csv.gz files inside the directory..."

                  FILES=$(aws s3api list-objects-v2 --bucket "$S3_BUCKET" --prefix "$LATEST_FOLDER" --query "Contents[?ends_with(Key, '.csv.gz')].Key" --output text)

                  if [ -z "$FILES" ]; then
                    echo "ERROR: No .csv.gz files found in $LATEST_FOLDER"
                    exit 1
                  fi

                  normalized=$(printf "%s" "$FILES" | tr '\t' '\n' | tr -d '\r' | sed '/^[[:space:]]*$/d')

                  # Convert into array safely
                  mapfile -t gzFilesList <<< "$normalized"

                  echo "Files after read:"
                  printf '%s\n' "${gzFilesList[@]}"

                  # Build SQL array
                  S3_KEY_ARRAY=$(
                    for key in "${gzFilesList[@]}"; do
                      safe_key=${key//\'/\'\'}
                      printf "'%s'," "$safe_key"
                    done | sed 's/,$//'
                  )

                  echo "[INFO $(date '+%Y-%m-%d %H:%M:%S')] Final SQL Array: ARRAY[$S3_KEY_ARRAY]"
                  echo

                  export DATABASE_URL="postgresql://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST/$POSTGRES_DBNAME"

                  echo "Running load from S3..."
                  psql "$DATABASE_URL" \
                    -c "SELECT run_aqi_s3_load('$S3_BUCKET', $S3_KEY_ARRAY, '$S3_REGION', NULL, '$AWS_ACCESS_KEY_ID', '$AWS_SECRET_ACCESS_KEY', '$AWS_SESSION_TOKEN');"

                  echo "Load from S3 completed."
                  echo "Running table swap..."
                  psql "$DATABASE_URL" \
                    -c "SELECT run_aqi_table_swap('AQI_AWS_S3_SYNC');"
                  echo "Table swap completed."
                  echo "AQI hourly sync completed."
          restartPolicy: OnFailure
